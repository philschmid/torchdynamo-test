{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ansor', 'aot_autograd', 'aot_cudagraphs', 'aot_eager', 'aot_inductor_debug', 'aot_print', 'aot_ts', 'aot_ts_nvfuser', 'aot_ts_nvfuser_nodecomps', 'cudagraphs', 'cudagraphs_ts', 'cudagraphs_ts_ofi', 'eager', 'fx2trt', 'inductor', 'ipex', 'nnc', 'nnc_ofi', 'nvprims_aten', 'nvprims_nvfuser', 'ofi', 'onednn', 'onnx2tensorrt', 'onnx2tensorrt_alt', 'onnx2tf', 'onnxrt', 'onnxrt_cpu', 'onnxrt_cpu_numpy', 'onnxrt_cuda', 'static_runtime', 'taso', 'tensorrt', 'torch2trt', 'torchxla_trace_once', 'torchxla_trivial', 'ts', 'ts_nvfuser', 'ts_nvfuser_ofi', 'tvm', 'tvm_meta_schedule']\n"
     ]
    }
   ],
   "source": [
    "import torch._dynamo as torchdynamo\n",
    "\n",
    "print(torchdynamo.list_backends())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torchdynamo.optimize(\"inductor\")\n",
    "def fn(x, y):\n",
    "    a = torch.cos(x)\n",
    "    b = torch.sin(y)\n",
    "    return a + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0823,  1.1241,  0.3413,  0.5874,  0.4794],\n",
       "        [-0.1441,  1.4301,  0.2987, -0.2521,  1.3261],\n",
       "        [ 0.7339,  0.0656,  0.3527,  1.9016, -0.0326]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 5)\n",
    "y = torch.randn(3, 5)\n",
    "\n",
    "fn(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1 µs ± 144 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "fn(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ResetRequired: \n",
    "Must call `torch._dynamo.reset()` before changing backends.  Detected two calls to\n",
    "`torch._dynamo.optimize(...)` with a different backend compiler arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torchdynamo.reset()\n",
    "\n",
    "@torchdynamo.optimize(\"fx2trt\")\n",
    "def fn(x, y):\n",
    "    a = torch.cos(x)\n",
    "    b = torch.sin(y)\n",
    "    return a + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 5)\n",
    "y = torch.randn(3, 5)\n",
    "\n",
    "fn(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ModuleNotFoundError: No module named 'torch_tensorrt'\n",
    "\n",
    "To use `fx2trt` backend, you need to install `torch-tensorrt` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torchdynamo.reset()\n",
    "\n",
    "@torchdynamo.optimize(\"onnxrt_cuda\")\n",
    "def fn(x, y):\n",
    "    a = torch.cos(x)\n",
    "    b = torch.sin(y)\n",
    "    return a + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 5)\n",
    "y = torch.randn(3, 5)\n",
    "\n",
    "fn(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ModuleNotFoundError: No module named 'onnxruntime'\n",
    "\n",
    "To use `onnxrt` backend, you need to install `onnxruntime` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62e32cb9823455e94fd5a9c0d659233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b278301e1c9c427eaa6525d5da382516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678c1417025640c9b5e6b3ba2a7ae1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114943affa614223b4bc744dfa3b80ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo as torchdynamo\n",
    "from torch._dynamo.optimizations import backends\n",
    "\n",
    "optimizer = torchdynamo.optimize(\"inductor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0001205497101182118, 0.9998794794082642]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload=\"I like you. I love\" * 21\n",
    "payload=\"I like you. I love\" \n",
    "\n",
    "input_ids = tokenizer(payload ,return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "\n",
    "# print payload lenght\n",
    "print(f\"length: {input_ids.shape}\")\n",
    "\n",
    "\n",
    "def vanilla_inf(input_ids):\n",
    "    with torch.inference_mode():\n",
    "      logits = model(input_ids=input_ids).logits.softmax(-1)[0]\n",
    "      pred = logits.tolist()\n",
    "      return pred\n",
    "\n",
    "vanilla_inf(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.22 ms ± 83.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "vanilla_inf(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0001205498119816184, 0.9998794794082642]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchdynamo.reset()\n",
    "\n",
    "\n",
    "@torchdynamo.optimize(\"inductor\")\n",
    "def dyn_inf(input_ids):\n",
    "    logits = model(input_ids=input_ids).logits.softmax(-1)[0]\n",
    "    pred = logits.tolist()\n",
    "    return pred\n",
    "    \n",
    "dyn_inf(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.07 ms ± 17.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dyn_inf(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "clf = pipeline(\"text-classification\",device=0)\n",
    "\n",
    "payload = \"I like you. I love you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.95 ms ± 70.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "clf(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchdynamo.reset()\n",
    "\n",
    "\n",
    "@torchdynamo.optimize(\"inductor\")\n",
    "def dyn_p(payload):\n",
    "    logits = clf(payload)\n",
    "    return logits\n",
    "    \n",
    "dyn_p(input_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ReferenceError: weakly-referenced object no longer exists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Abstract model class for TorchDynamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3282,  4.6951]], device='cuda:0',\n",
       "       grad_fn=<CompiledFunctionBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch._dynamo as torchdynamo\n",
    "\n",
    "\n",
    "class TorchDynamoModel(nn.Module):\n",
    "    def __init__(self, transformer: nn.Module = None, optimizer: str = None):\n",
    "        super(TorchDynamoModel, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.config = transformer.config\n",
    "        self.optimizer = optimizer\n",
    "        self.forward = self.create_optimized_forward(optimizer)\n",
    "    \n",
    "    def create_optimized_forward(self, optimizer):\n",
    "      torchdynamo.reset()\n",
    "\n",
    "      @torchdynamo.optimize(optimizer)\n",
    "      def new_fwd( *args, **kwargs):\n",
    "        return self.transformer(*args, **kwargs)\n",
    "      \n",
    "      return new_fwd\n",
    "    \n",
    "\n",
    "trt = TorchDynamoModel(transformer=model,optimizer=\"inductor\")\n",
    "trt(input_ids)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.92 ms ± 27.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "trt(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.43 ms ± 153 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pipeline with `TorchDynamoModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_clf = pipeline(\"text-classification\", model=trt, tokenizer=tokenizer, framework=\"pt\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.08 ms ± 13.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dyn_clf(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 ms ± 75.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "clf(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it will \"re-optimize\" once the input length changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyn_clf(\"I hate you\")\n",
    "# 15s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyn_clf(\"I hate you\")\n",
    "# 0.2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accelerate approach \n",
    "\n",
    "based on https://github.com/huggingface/accelerate/pull/829/files#diff-2d7515874eaecac2687c7fc1a9c720be53f802bf14b4c3dcebe14ad443d075dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo as dynamo\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\").to(device)\n",
    "\n",
    "optimizer = \"inductor\"\n",
    "o_model = dynamo.optimize(optimizer)(model)\n",
    "\n",
    "o_clf = pipeline(\"text-classification\", model=o_model, tokenizer=tokenizer, framework=\"pt\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._dynamo.eval_frame._TorchDynamoContext.__call__.<locals>.TorchDynamoNNModuleWrapper at 0x7f368d2a0070>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload=\"I like you. I love you\" \n",
    "\n",
    "input_ids = tokenizer(payload ,return_tensors=\"pt\")[\"input_ids\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477 µs ± 113 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "o_model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.86 ms ± 18.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "o_clf(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "| kind                             | model latency | pipeline latency | payload              |\n",
    "|----------------------------------|---------------|------------------|----------------------|\n",
    "| vanilla transformers             | 4.22ms        | 5ms              | \"I like you. I love\" |\n",
    "| TorchDynamoModel (wrapped fwd)   | 1.92ms        | 2.08ms           | \"I like you. I love\" |\n",
    "| accelerate (_TorchDynamoContext) | 1.93ms        | 4.95ms           | \"I like you. I love\" |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('conda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

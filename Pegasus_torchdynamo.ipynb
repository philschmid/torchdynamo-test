{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Torchdynamo for improving Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 11 12:46:10 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   31C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "def measure_latency(payload,model, tokenizer,num_beams=1):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(2):\n",
    "        _ = generate_from_model(payload,model, tokenizer,num_beams)\n",
    "    # Timed run\n",
    "    for _ in range(50):\n",
    "        start_time = perf_counter()\n",
    "        _ =  generate_from_model(payload,model, tokenizer,num_beams)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms\n",
    "\n",
    "def generate_from_model(payload,model, tokenizer,num_beams=1):\n",
    "    encoded_input = tokenizer(payload, return_tensors='pt',pad_to_multiple_of=8)\n",
    "    output_sequences = model.generate(input_ids=encoded_input['input_ids'].to(device),num_beams=num_beams)\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SOtcOWsOl2RY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"RobertoFont/pegasus-large-samsum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"RobertoFont/pegasus-large-samsum\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8RJ1j4klzQe",
    "outputId": "e9751395-6e8c-490d-8fec-2b53d8653e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload token length 1023\n"
     ]
    }
   ],
   "source": [
    "payload=\"Renee: Just saying Hi. Thought of you this morning.Layla misses you. She is having knee surgery. Hope you are doing well. Rachel: Renee! Hey! Whoa! So crazy that you wrote. I was literally thinking of you the other day as well. Rachel: I am doing really well, getting settled here and everything. Looking for a new job. Rachel: Layla is getting knew surgery?! What happened? Renee: Her arthritis got really bad. I saw her limping every time we went out for a walk, and so I took her to the vet and they decided she needs surgery. Rachel: Oh wow. That is really intense. I am sending her lots of love. Renee: Thanks Rachel. Renee: Here is a pic of Layla from this morning. Renee: <file_picture> Renee: She is all cute in her usual spot. Rachel: Ha! Yeah, I remember, she would always try to fit in there even though she's obviously too big Rachel: ðŸ˜‚ Renee: Yeah, she's always getting into some sort of silly situations. Renee: What kind of work are you looking for? Rachel: Just the usual, something with teaching. I am not too stressed yet, I have some savings. Renee: That's good, yeah, you don't want to have to feel pressured into taking a certain job. Renee: Just so you know, if you are ever back, you are more than welcome to come back and work for me. Renee: Layla is definitely missing you taking her on walks everyday. Rachel: Aw, yeah, I miss you guys too. Of course If I am ever back I am\" * 3\n",
    "\n",
    "print(f\"Payload token length {len(tokenizer(payload)['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "RNJV1j7SmDRP",
    "outputId": "f327f966-1f59-4a00-8317-cc89c13141b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Renee's dog Layla is having knee surgery. Rachel is looking for a new job.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(payload ,return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "logits = model.generate(input_ids=input_ids)\n",
    "tokenizer.decode(logits[0],skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "0xsOpUCTnp_O",
    "outputId": "52a7064f-e016-4997-ff1a-5e2152b2c508"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P95 latency (ms) - 464.1055683999639; Average latency (ms) - 455.84 +\\\\- 5.26;'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_latency(payload,model,tokenizer)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P95 latency (ms) - 681.1002442500694; Average latency (ms) - 674.37 +\\\\- 3.89;'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_latency(payload,model,tokenizer,num_beams=5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kudo0edqhKw"
   },
   "source": [
    "**Greedy Search**\n",
    "```\n",
    "GPU: 'P95 latency (ms) - 464.1055683999639; Average latency (ms) - 455.84 +\\\\- 5.26;'\n",
    "TorchDynamo: \n",
    "```\n",
    "\n",
    "**Beam Search (5)**\n",
    "\n",
    "```\n",
    "GPU: 'P95 latency (ms) - 681.1002442500694; Average latency (ms) - 674.37 +\\\\- 3.89;'\n",
    "TorchDynamo: \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ez5XDcUq6J7"
   },
   "source": [
    "# Torchdynamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uHNECpNPq9Z2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchdynamo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# list backends \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchdynamo\u001b[39;00m\n\u001b[1;32m      4\u001b[0m torchdynamo\u001b[38;5;241m.\u001b[39mlist_backends()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchdynamo'"
     ]
    }
   ],
   "source": [
    "# list backends \n",
    "import torchdynamo\n",
    "\n",
    "torchdynamo.list_backends()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get backend/optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbYm3-l6rUvP"
   },
   "outputs": [],
   "source": [
    "from torchdynamo.optimizations import backends\n",
    "dynamo_fx2trt_fp32\n",
    "optimizer = torchdynamo.optimize(backends.fx2trt_compiler_fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKNJUk9kvk_-"
   },
   "outputs": [],
   "source": [
    "optimized_mod = torchdynamo.optimize(optimizer)(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_latency(payload,optimized_mod,tokenizer)[0]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Pegasus torchdynamo",
   "provenance": []
  },
  "gpuClass": "standard",
  "interpreter": {
   "hash": "3e40e9f08dfc2ade1dcbbad55f529cae4cddafd508eff01e995ff504a717cc52"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
